{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93fea5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ ƒë·ªçc 5,416 d√≤ng d·ªØ li·ªáu\n",
      "\n",
      "C√°c c·ªôt: ['domain', 'review_id', 'sentence_id', 'text', 'aspect', 'polarity', 'label']\n",
      "\n",
      "Ph√¢n b·ªë polarity:\n",
      "polarity\n",
      "positive    3294\n",
      "negative    1833\n",
      "neutral      289\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ƒê·ªçc file g·ªëc\n",
    "df = pd.read_csv('h:/SentimentAnalystSchool/MidtermExam/Semeval/df_train_all.csv')\n",
    "\n",
    "print(f\"ƒê√£ ƒë·ªçc {len(df):,} d√≤ng d·ªØ li·ªáu\")\n",
    "print(f\"\\nC√°c c·ªôt: {df.columns.tolist()}\")\n",
    "print(f\"\\nPh√¢n b·ªë polarity:\")\n",
    "print(df['polarity'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5697df0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T·ªïng s·ªë m·∫´u ban ƒë·∫ßu: 5,416\n",
      "\n",
      "Ph√¢n b·ªë Sentiment ban ƒë·∫ßu:\n",
      "Sentiment\n",
      "positive    3294\n",
      "negative    1833\n",
      "neutral      289\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# C·∫§U H√åNH S·ªê L∆Ø·ª¢NG M·∫™U THEO SENTIMENT\n",
    "# ============================================\n",
    "# Ch·ªânh s·ª≠a c√°c gi√° tr·ªã b√™n d∆∞·ªõi ƒë·ªÉ thay ƒë·ªïi s·ªë l∆∞·ª£ng m·∫´u cho m·ªói nh√£n\n",
    "# ƒê·∫∑t None n·∫øu mu·ªën l·∫•y t·∫•t c·∫£ d·ªØ li·ªáu c·ªßa nh√£n ƒë√≥\n",
    "\n",
    "BALANCE_CONFIG = {\n",
    "    'positive': 500,   # S·ªë l∆∞·ª£ng m·∫´u positive t·ªëi ƒëa\n",
    "    'negative': 500,   # S·ªë l∆∞·ª£ng m·∫´u negative t·ªëi ƒëa  \n",
    "    'neutral': 289     # S·ªë l∆∞·ª£ng m·∫´u neutral t·ªëi ƒëa\n",
    "}\n",
    "\n",
    "# ƒê·∫∑t BALANCE_CONFIG = None n·∫øu mu·ªën gi·ªØ t·∫•t c·∫£ d·ªØ li·ªáu\n",
    "# BALANCE_CONFIG = None\n",
    "\n",
    "# ============================================\n",
    "\n",
    "# T·∫°o DataFrame v·ªõi 2 c·ªôt: reviewText v√† Sentiment\n",
    "df_export = pd.DataFrame({\n",
    "    'reviewText': df['text'],\n",
    "    'Sentiment': df['polarity']\n",
    "})\n",
    "\n",
    "print(f\"T·ªïng s·ªë m·∫´u ban ƒë·∫ßu: {len(df_export):,}\")\n",
    "print(f\"\\nPh√¢n b·ªë Sentiment ban ƒë·∫ßu:\")\n",
    "print(df_export['Sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e682ee4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "√ÅP D·ª§NG C√ÇN B·∫∞NG D·ªÆ LI·ªÜU\n",
      "============================================================\n",
      "  positive: 3,294 ‚Üí 500 (gi·∫£m 2,794)\n",
      "  negative: 1,833 ‚Üí 500 (gi·∫£m 1,333)\n",
      "  neutral: 289 (gi·ªØ nguy√™n)\n",
      "\n",
      "üìä T·ªïng s·ªë m·∫´u sau khi x·ª≠ l√Ω: 1,289\n",
      "\n",
      "üìà Ph√¢n b·ªë Sentiment cu·ªëi c√πng:\n",
      "Sentiment\n",
      "positive    500\n",
      "negative    500\n",
      "neutral     289\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# √Åp d·ª•ng c√¢n b·∫±ng d·ªØ li·ªáu n·∫øu c√≥ BALANCE_CONFIG\n",
    "if BALANCE_CONFIG:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"√ÅP D·ª§NG C√ÇN B·∫∞NG D·ªÆ LI·ªÜU\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    balanced_dfs = []\n",
    "    for sentiment, max_count in BALANCE_CONFIG.items():\n",
    "        sentiment_df = df_export[df_export['Sentiment'] == sentiment]\n",
    "        current_count = len(sentiment_df)\n",
    "        \n",
    "        if max_count is not None and current_count > max_count:\n",
    "            sentiment_df = sentiment_df.sample(n=max_count, random_state=42)\n",
    "            print(f\"  {sentiment}: {current_count:,} ‚Üí {max_count:,} (gi·∫£m {current_count - max_count:,})\")\n",
    "        else:\n",
    "            print(f\"  {sentiment}: {current_count:,} (gi·ªØ nguy√™n)\")\n",
    "        \n",
    "        balanced_dfs.append(sentiment_df)\n",
    "    \n",
    "    df_final = pd.concat(balanced_dfs, ignore_index=True)\n",
    "    # Shuffle l·∫°i\n",
    "    df_final = df_final.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "else:\n",
    "    df_final = df_export\n",
    "\n",
    "print(f\"\\nüìä T·ªïng s·ªë m·∫´u sau khi x·ª≠ l√Ω: {len(df_final):,}\")\n",
    "print(f\"\\nüìà Ph√¢n b·ªë Sentiment cu·ªëi c√πng:\")\n",
    "print(df_final['Sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d736282f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ export 1,289 d√≤ng ra file: h:/SentimentAnalystSchool/MidtermExam/Semeval/semeval_500_500_289.csv\n",
      "\n",
      "M·∫´u d·ªØ li·ªáu ƒë·∫ßu ti√™n:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The keyboard is a little wonky with having to ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm still exploring the various features but o...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Just what the doctor ordered.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quite simply this is the best laptop I have ev...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The farro salad and the mashed yukon potatoes ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I highly recommend it.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I love my Apple, it is quick and easy to use.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>while the keyboard itself is alright, the plat...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>This laptop is very large and barely fits in a...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The cover is a soft rubber texture without the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText Sentiment\n",
       "0  The keyboard is a little wonky with having to ...   neutral\n",
       "1  I'm still exploring the various features but o...   neutral\n",
       "2                      Just what the doctor ordered.  positive\n",
       "3  Quite simply this is the best laptop I have ev...  positive\n",
       "4  The farro salad and the mashed yukon potatoes ...  positive\n",
       "5                             I highly recommend it.  positive\n",
       "6      I love my Apple, it is quick and easy to use.  positive\n",
       "7  while the keyboard itself is alright, the plat...   neutral\n",
       "8  This laptop is very large and barely fits in a...  negative\n",
       "9  The cover is a soft rubber texture without the...  positive"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export ra file CSV\n",
    "output_path = 'h:/SentimentAnalystSchool/MidtermExam/Semeval/semeval_500_500_289.csv'\n",
    "df_final.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"‚úì ƒê√£ export {len(df_final):,} d√≤ng ra file: {output_path}\")\n",
    "print(f\"\\nM·∫´u d·ªØ li·ªáu ƒë·∫ßu ti√™n:\")\n",
    "df_final.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a93ba39",
   "metadata": {},
   "source": [
    "## Benchmark v·ªõi Model DistilBERT + LSTM\n",
    "\n",
    "S·ª≠ d·ª•ng model ƒë√£ train ƒë·ªÉ benchmark tr√™n b·ªô d·ªØ li·ªáu Semeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2572377f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt cho benchmark\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ki·ªÉm tra GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f5870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ƒê·ªãnh nghƒ©a Model Architecture (ph·∫£i gi·ªëng v·ªõi model ƒë√£ train)\n",
    "class DistilBertLSTMClassifier(nn.Module):\n",
    "    def __init__(self, n_classes=3, lstm_hidden_size=128, lstm_layers=2, dropout=0.3):\n",
    "        super(DistilBertLSTMClassifier, self).__init__()\n",
    "        \n",
    "        # DistilBERT\n",
    "        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        \n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=768,\n",
    "            hidden_size=lstm_hidden_size,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if lstm_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Classifier\n",
    "        self.fc = nn.Linear(lstm_hidden_size * 2, n_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        distilbert_output = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        sequence_output = distilbert_output.last_hidden_state\n",
    "        lstm_output, (hidden, cell) = self.lstm(sequence_output)\n",
    "        hidden_fwd = hidden[-2, :, :]\n",
    "        hidden_bwd = hidden[-1, :, :]\n",
    "        hidden_concat = torch.cat((hidden_fwd, hidden_bwd), dim=1)\n",
    "        hidden_concat = self.dropout(hidden_concat)\n",
    "        output = self.fc(hidden_concat)\n",
    "        return output\n",
    "\n",
    "print(\"‚úì Model architecture defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d764831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class cho benchmark\n",
    "class BenchmarkDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len=128):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.data.loc[idx, 'text'])\n",
    "        aspect = str(self.data.loc[idx, 'aspect'])\n",
    "        label = self.data.loc[idx, 'label']\n",
    "        \n",
    "        # K·∫øt h·ª£p text v√† aspect\n",
    "        combined_text = text + \" [SEP] \" + aspect\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            combined_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"‚úì Dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb879680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model weights\n",
    "MODEL_PATH = 'h:/SentimentAnalystSchool/MidtermExam/ModelWeight/distilbert_lstm_model_complete.pt'\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(MODEL_PATH, map_location=device)\n",
    "\n",
    "# Kh·ªüi t·∫°o model\n",
    "model = DistilBertLSTMClassifier(n_classes=3)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load label encoder t·ª´ checkpoint\n",
    "label_encoder = checkpoint['label_encoder']\n",
    "MAX_LEN = checkpoint.get('max_len', 128)\n",
    "\n",
    "print(\"‚úì Model loaded successfully\")\n",
    "print(f\"Label classes: {label_encoder.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12294e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load v√† chu·∫©n b·ªã d·ªØ li·ªáu benchmark\n",
    "df_benchmark = pd.read_csv('h:/SentimentAnalystSchool/MidtermExam/Semeval/df_train_all.csv')\n",
    "\n",
    "print(f\"Loaded {len(df_benchmark):,} samples for benchmark\")\n",
    "print(f\"\\nPolarity distribution:\")\n",
    "print(df_benchmark['polarity'].value_counts())\n",
    "\n",
    "# Encode labels (s·ª≠ d·ª•ng label encoder t·ª´ model)\n",
    "# Map polarity sang label number\n",
    "polarity_to_label = {label: idx for idx, label in enumerate(label_encoder.classes_)}\n",
    "df_benchmark['label'] = df_benchmark['polarity'].map(polarity_to_label)\n",
    "\n",
    "# Ki·ªÉm tra xem c√≥ gi√° tr·ªã n√†o kh√¥ng map ƒë∆∞·ª£c kh√¥ng\n",
    "missing_labels = df_benchmark['label'].isna().sum()\n",
    "if missing_labels > 0:\n",
    "    print(f\"\\n‚ö† Warning: {missing_labels} samples c√≥ polarity kh√¥ng kh·ªõp v·ªõi label encoder\")\n",
    "    print(f\"Polarity trong data: {df_benchmark['polarity'].unique()}\")\n",
    "    print(f\"Labels trong model: {label_encoder.classes_}\")\n",
    "\n",
    "# B·ªè c√°c d√≤ng kh√¥ng map ƒë∆∞·ª£c\n",
    "df_benchmark = df_benchmark.dropna(subset=['label'])\n",
    "df_benchmark['label'] = df_benchmark['label'].astype(int)\n",
    "\n",
    "print(f\"\\nSamples sau khi x·ª≠ l√Ω: {len(df_benchmark):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1988ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o DataLoader cho benchmark\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "benchmark_dataset = BenchmarkDataset(df_benchmark, tokenizer, MAX_LEN)\n",
    "benchmark_loader = DataLoader(benchmark_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"‚úì DataLoader created with {len(benchmark_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667e6b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H√†m benchmark\n",
    "def run_benchmark(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Ch·∫°y benchmark tr√™n b·ªô d·ªØ li·ªáu\n",
    "    \n",
    "    Returns:\n",
    "        accuracy, all_preds, all_labels\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"RUNNING BENCHMARK\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            if (batch_idx + 1) % 50 == 0:\n",
    "                print(f\"  Processed {batch_idx + 1}/{len(data_loader)} batches...\")\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return accuracy, all_preds, all_labels\n",
    "\n",
    "print(\"‚úì Benchmark function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f007c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch·∫°y benchmark\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy, all_preds, all_labels = run_benchmark(model, benchmark_loader, device)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"BENCHMARK RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n‚è± Time: {elapsed_time:.2f} seconds\")\n",
    "print(f\"üìä Total samples: {len(all_labels):,}\")\n",
    "print(f\"üéØ Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be97f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi ti·∫øt classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "label_names = ['Negative', 'Neutral', 'Positive']\n",
    "report = classification_report(all_labels, all_preds, target_names=label_names)\n",
    "print(\"üìã Classification Report:\")\n",
    "print(\"-\" * 60)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ee2554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# T·∫°o confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 1. Confusion Matrix v·ªõi s·ªë l∆∞·ª£ng\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=label_names, yticklabels=label_names, ax=axes[0])\n",
    "axes[0].set_title('Confusion Matrix (Counts)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "# 2. Confusion Matrix v·ªõi ph·∫ßn trƒÉm (normalized)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.1f', cmap='Blues',\n",
    "            xticklabels=label_names, yticklabels=label_names, ax=axes[1])\n",
    "axes[1].set_title('Confusion Matrix (Percentage %)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('benchmark_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüíæ Confusion matrix saved to 'benchmark_confusion_matrix.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202ec63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Th·ªëng k√™ chi ti·∫øt theo t·ª´ng class\n",
    "print(\"üìä Detailed Statistics per Class:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, label in enumerate(label_names):\n",
    "    total = cm[i].sum()\n",
    "    correct = cm[i][i]\n",
    "    incorrect = total - correct\n",
    "    class_acc = (correct / total * 100) if total > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{label.upper()}:\")\n",
    "    print(f\"  ‚Ä¢ Total samples: {total:,}\")\n",
    "    print(f\"  ‚Ä¢ Correct predictions: {correct:,} ({class_acc:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Incorrect predictions: {incorrect:,} ({100-class_acc:.1f}%)\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìà OVERALL SUMMARY:\")\n",
    "print(f\"  ‚Ä¢ Dataset: Semeval (df_train_all.csv)\")\n",
    "print(f\"  ‚Ä¢ Model: DistilBERT + LSTM\")\n",
    "print(f\"  ‚Ä¢ Weight file: distilbert_lstm_model_complete.pt\")\n",
    "print(f\"  ‚Ä¢ Overall Accuracy: {accuracy*100:.2f}%\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
