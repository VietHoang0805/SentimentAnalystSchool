{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a4cef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# ÄÆ°á»ng dáº«n gá»‘c\n",
    "BASE_PATH = Path(\"H:/SentimentAnalystSchool/MidtermExam/DatasetUnder80kWord\")\n",
    "\n",
    "# Äá»‹nh nghÄ©a cÃ¡c file cáº§n gá»™p\n",
    "FILES_CONFIG = {\n",
    "    \"Amazon\": {\n",
    "        \"train\": BASE_PATH / \"Amazon\" / \"train_reviews_under80words.csv\",\n",
    "        \"dev\": BASE_PATH / \"Amazon\" / \"dev_reviews_under80words.csv\",\n",
    "        \"test\": BASE_PATH / \"Amazon\" / \"test_reviews_under80words.csv\"\n",
    "    },\n",
    "    \"Hotels\": {\n",
    "        \"train\": BASE_PATH / \"Hotels\" / \"train_reviews_under80words.csv\",\n",
    "        \"dev\": BASE_PATH / \"Hotels\" / \"dev_reviews_under80words.csv\",\n",
    "        \"test\": BASE_PATH / \"Hotels\" / \"test_reviews_under80words.csv\"\n",
    "    },\n",
    "    \"AmazonsElectronics\": {\n",
    "        \"all\": BASE_PATH / \"AmazonsElectronics\" / \"output_with_aspects.csv\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Cáº¥u hÃ¬nh Ä‘Æ°á»ng dáº«n files:\")\n",
    "for source, files in FILES_CONFIG.items():\n",
    "    print(f\"\\n{source}:\")\n",
    "    for file_type, path in files.items():\n",
    "        exists = \"âœ“\" if path.exists() else \"âœ—\"\n",
    "        print(f\"  {file_type}: {path.name} [{exists}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5acd404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_standardize_data(file_path, source_name, file_type=\"train\"):\n",
    "    \"\"\"\n",
    "    Äá»c file CSV vÃ  chuáº©n hÃ³a cá»™t\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: Ä‘Æ°á»ng dáº«n file\n",
    "    - source_name: tÃªn nguá»“n dá»¯ liá»‡u (Amazon, Hotels, etc.)\n",
    "    - file_type: loáº¡i file (train, dev, test, all)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame Ä‘Ã£ chuáº©n hÃ³a\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Chuáº©n hÃ³a tÃªn cá»™t\n",
    "        df.columns = df.columns.str.strip().str.lower()\n",
    "        \n",
    "        # Map cÃ¡c tÃªn cá»™t cÃ³ thá»ƒ khÃ¡c nhau\n",
    "        column_mapping = {\n",
    "            'reviewtext': 'text',\n",
    "            'review': 'text',\n",
    "            'aspectterm': 'aspect',\n",
    "            'aspect': 'aspect',\n",
    "            'sentiment': 'sentiment'\n",
    "        }\n",
    "        \n",
    "        df = df.rename(columns=column_mapping)\n",
    "        \n",
    "        # ThÃªm cá»™t metadata\n",
    "        df['source'] = source_name\n",
    "        df['split'] = file_type\n",
    "        \n",
    "        # Chá»‰ giá»¯ cÃ¡c cá»™t cáº§n thiáº¿t\n",
    "        required_cols = ['text', 'aspect', 'sentiment', 'source', 'split']\n",
    "        available_cols = [col for col in required_cols if col in df.columns]\n",
    "        df = df[available_cols]\n",
    "        \n",
    "        print(f\"  âœ“ Loaded {len(df)} rows from {file_path.name}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Error loading {file_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "print(\"HÃ m load_and_standardize_data Ä‘Ã£ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab62276e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_datasets(files_config, output_path=None):\n",
    "    \"\"\"\n",
    "    Gá»™p táº¥t cáº£ cÃ¡c dataset thÃ nh má»™t file duy nháº¥t\n",
    "    \n",
    "    Parameters:\n",
    "    - files_config: dict chá»©a cáº¥u hÃ¬nh cÃ¡c file\n",
    "    - output_path: Ä‘Æ°á»ng dáº«n lÆ°u file output (optional)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame Ä‘Ã£ gá»™p\n",
    "    \"\"\"\n",
    "    all_dataframes = []\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"Báº®T Äáº¦U Gá»˜P DATASETS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for source_name, files in files_config.items():\n",
    "        print(f\"\\nğŸ“‚ Loading from {source_name}:\")\n",
    "        for file_type, file_path in files.items():\n",
    "            if file_path.exists():\n",
    "                df = load_and_standardize_data(file_path, source_name, file_type)\n",
    "                if not df.empty:\n",
    "                    all_dataframes.append(df)\n",
    "            else:\n",
    "                print(f\"  âš  File not found: {file_path}\")\n",
    "    \n",
    "    if not all_dataframes:\n",
    "        print(\"\\nâŒ KhÃ´ng cÃ³ dá»¯ liá»‡u nÃ o Ä‘Æ°á»£c load!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Gá»™p táº¥t cáº£\n",
    "    merged_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Káº¾T QUáº¢ Gá»˜P DATASET\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nğŸ“Š Tá»•ng sá»‘ máº«u: {len(merged_df):,}\")\n",
    "    print(f\"\\nğŸ“ˆ PhÃ¢n bá»‘ theo nguá»“n:\")\n",
    "    print(merged_df['source'].value_counts().to_string())\n",
    "    print(f\"\\nğŸ“ˆ PhÃ¢n bá»‘ theo split:\")\n",
    "    print(merged_df['split'].value_counts().to_string())\n",
    "    print(f\"\\nğŸ“ˆ PhÃ¢n bá»‘ theo sentiment:\")\n",
    "    print(merged_df['sentiment'].value_counts().to_string())\n",
    "    \n",
    "    # LÆ°u file náº¿u cÃ³ output_path\n",
    "    if output_path:\n",
    "        merged_df.to_csv(output_path, index=False)\n",
    "        print(f\"\\nğŸ’¾ ÄÃ£ lÆ°u file: {output_path}\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "print(\"HÃ m merge_all_datasets Ä‘Ã£ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240f011c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thá»±c hiá»‡n gá»™p dataset\n",
    "OUTPUT_PATH = BASE_PATH / \"merged_dataset_all.csv\"\n",
    "\n",
    "merged_df = merge_all_datasets(FILES_CONFIG, output_path=OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f236f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xem má»™t sá»‘ máº«u dá»¯ liá»‡u\n",
    "print(\"ğŸ“‹ Má»™t sá»‘ máº«u dá»¯ liá»‡u Ä‘áº§u tiÃªn:\")\n",
    "merged_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597d82de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_merged_dataset(merged_df, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, \n",
    "                         output_dir=None, random_state=42):\n",
    "    \"\"\"\n",
    "    Chia dataset Ä‘Ã£ gá»™p thÃ nh train/val/test\n",
    "    \n",
    "    Parameters:\n",
    "    - merged_df: DataFrame Ä‘Ã£ gá»™p\n",
    "    - train_ratio, val_ratio, test_ratio: tá»‰ lá»‡ chia\n",
    "    - output_dir: thÆ° má»¥c lÆ°u file (optional)\n",
    "    - random_state: seed cho random\n",
    "    \n",
    "    Returns:\n",
    "    - train_df, val_df, test_df\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Shuffle data\n",
    "    df = merged_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    # Chia train vÃ  temp (val + test)\n",
    "    train_df, temp_df = train_test_split(\n",
    "        df, \n",
    "        train_size=train_ratio,\n",
    "        random_state=random_state,\n",
    "        stratify=df['sentiment'] if 'sentiment' in df.columns else None\n",
    "    )\n",
    "    \n",
    "    # Chia temp thÃ nh val vÃ  test\n",
    "    val_size = val_ratio / (val_ratio + test_ratio)\n",
    "    val_df, test_df = train_test_split(\n",
    "        temp_df,\n",
    "        train_size=val_size,\n",
    "        random_state=random_state,\n",
    "        stratify=temp_df['sentiment'] if 'sentiment' in temp_df.columns else None\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Káº¿t quáº£ chia dataset:\")\n",
    "    print(f\"  Train: {len(train_df):,} máº«u ({train_ratio*100:.0f}%)\")\n",
    "    print(f\"  Val:   {len(val_df):,} máº«u ({val_ratio*100:.0f}%)\")\n",
    "    print(f\"  Test:  {len(test_df):,} máº«u ({test_ratio*100:.0f}%)\")\n",
    "    \n",
    "    # LÆ°u file náº¿u cÃ³ output_dir\n",
    "    if output_dir:\n",
    "        output_dir = Path(output_dir)\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        train_df.to_csv(output_dir / \"train_merged.csv\", index=False)\n",
    "        val_df.to_csv(output_dir / \"val_merged.csv\", index=False)\n",
    "        test_df.to_csv(output_dir / \"test_merged.csv\", index=False)\n",
    "        \n",
    "        print(f\"\\nğŸ’¾ ÄÃ£ lÆ°u files vÃ o: {output_dir}\")\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "print(\"HÃ m split_merged_dataset Ä‘Ã£ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2969ae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chia dataset thÃ nh train/val/test vá»›i tá»‰ lá»‡ 80/10/10\n",
    "OUTPUT_DIR = BASE_PATH / \"merged\"\n",
    "\n",
    "train_df, val_df, test_df = split_merged_dataset(\n",
    "    merged_df, \n",
    "    train_ratio=0.8, \n",
    "    val_ratio=0.1, \n",
    "    test_ratio=0.1,\n",
    "    output_dir=OUTPUT_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41157acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiá»ƒm tra phÃ¢n bá»‘ sentiment trong tá»«ng split\n",
    "print(\"ğŸ“Š PhÃ¢n bá»‘ sentiment trong tá»«ng split:\\n\")\n",
    "\n",
    "for name, df in [(\"Train\", train_df), (\"Val\", val_df), (\"Test\", test_df)]:\n",
    "    print(f\"{name}:\")\n",
    "    print(df['sentiment'].value_counts(normalize=True).apply(lambda x: f\"{x:.2%}\"))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86322249",
   "metadata": {},
   "source": [
    "## Tá»•ng káº¿t\n",
    "\n",
    "CÃ¡c file Ä‘Ã£ Ä‘Æ°á»£c táº¡o:\n",
    "- `merged_dataset_all.csv`: File gá»™p táº¥t cáº£ dá»¯ liá»‡u\n",
    "- `merged/train_merged.csv`: Táº­p train (80%)\n",
    "- `merged/val_merged.csv`: Táº­p validation (10%)  \n",
    "- `merged/test_merged.csv`: Táº­p test (10%)\n",
    "\n",
    "Cáº¥u trÃºc cá»™t cá»§a dataset Ä‘Ã£ chuáº©n hÃ³a:\n",
    "- `text`: Ná»™i dung review\n",
    "- `aspect`: Aspect term Ä‘Æ°á»£c trÃ­ch xuáº¥t\n",
    "- `sentiment`: NhÃ£n sentiment (positive/negative/neutral)\n",
    "- `source`: Nguá»“n dá»¯ liá»‡u (Amazon/Hotels/AmazonsElectronics)\n",
    "- `split`: Loáº¡i split gá»‘c (train/dev/test/all)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
